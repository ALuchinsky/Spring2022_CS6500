{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3aff68-9186-488e-b9ef-57e65911b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import format_number, mean, min, max, corr, stddev\n",
    "from pyspark.sql.functions import (dayofmonth, hour, dayofyear, month, year, weekofyear, format_number, date_format, asc, desc)\n",
    "from pyspark.sql.functions import explode, col, element_at, size, split\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba777c2-dfa9-44d7-aff2-dc9a99bcf662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a SparkSession named as \"test123\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('test_123') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
    "    .config('spark.sql.session.timeZone', 'UTC') \\\n",
    "    .config('spark.driver.memory','8g') \\\n",
    "    .config('spark.ui.showConsoleProgress', True) \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf270b6-a032-421b-9a7c-cc8265bd3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parh = \"/papers_2000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5fbe6f-e226-42ad-8403-880cea2d418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_papers = spark.read.json(\"../data/processed/\"+data_parh+\"/short_papers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3628fbc-5950-4be3-8be2-773fc6de9f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_papers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f624d3c-408f-4600-a3ff-c531066be140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624fb40f-5df1-4176-a4f5-1409ab40d674",
   "metadata": {},
   "source": [
    "This is the list of assigned keywords from the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c0c45f-bfc5-483f-b120-c2672c32fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "kws  = short_papers.select(explode(col(\"keywords\"))).select(\"col.value\").withColumnRenamed(\"value\", \"K\")\n",
    "kws_counts = kws.groupby(\"K\").count().sort(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44938f0-1579-4d44-9f4e-11322dec5943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- citation_count: long (nullable = true)\n",
      " |-- created: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- schema: string (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |-- num_refs: long (nullable = true)\n",
      " |-- number_of_pages: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "short_papers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfbe3eb-60cd-413b-8c25-0471b45f2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kws_counts = kws_counts.limit(1000).toPandas()[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8329682-ebee-4581-9823-7a1de0a89269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_kws_counts.plot(style = \"-\")\n",
    "plt.axvline(20, linestyle = \"--\")\n",
    "#plt.axvline(100, linestyle = \"--\")\n",
    "plt.xlim(0, 200)\n",
    "plt.xlabel(\"Number of papers\")\n",
    "plt.ylabel(\"Number of usages\")\n",
    "plt.savefig(\"figures/assigned_kws_elbow.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a190a4-7eb4-4e9f-930a-54d89ce9f343",
   "metadata": {},
   "source": [
    "Select 20 or 100 most popular  words should be used. We will work with 20 for a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60eb3795-bb55-4636-97cd-73b2e3edc526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 1 & numerical calculations & 3031 \\\\ \n",
      "\t 2 & numerical calculations: interpretation of experiments & 1240 \\\\ \n",
      "\t 3 & bibliography & 982 \\\\ \n",
      "\t 4 & quantum chromodynamics & 876 \\\\ \n",
      "\t 5 & supersymmetry & 778 \\\\ \n",
      "\t 6 & CP: violation & 610 \\\\ \n",
      "\t 7 & neutrino: oscillation & 602 \\\\ \n",
      "\t 8 & Feynman graph & 573 \\\\ \n",
      "\t 9 & electron positron: annihilation & 542 \\\\ \n",
      "\t 10 & Feynman graph: higher-order & 459 \\\\ \n",
      "\t 11 & effective Lagrangian & 429 \\\\ \n",
      "\t 12 & neutrino: mixing angle & 395 \\\\ \n",
      "\t 13 & neutrino: mass & 389 \\\\ \n",
      "\t 14 & neutrino: solar & 373 \\\\ \n",
      "\t 15 & perturbation theory: higher-order & 369 \\\\ \n",
      "\t 16 & Higgs particle: mass & 341 \\\\ \n",
      "\t 17 & electroweak interaction & 307 \\\\ \n",
      "\t 18 & neutrino: cosmic radiation & 293 \\\\ \n",
      "\t 19 & p p: inclusive reaction & 283 \\\\ \n",
      "\t 20 & critical phenomena & 282 \\\\ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tab_data = kws_counts.limit(20).collect()\n",
    "tex = \"\"\n",
    "for i, r in enumerate(tab_data):\n",
    "#    print(i,\" \", r[0], r[1])\n",
    "    tex = tex + \"\\t \"+str(i+1) + \" & \" + r[0] + \" & \" + str(r[1]) + r\" \\\\ \" + \"\\n\"\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5dab93-154d-4024-bb97-cde6c77192be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing some characters with _\n",
    "import re\n",
    "def format_kwd(kwd):\n",
    "    return re.sub(\"[\\.-/ :()]\", \"_\", kwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d60a231-51e9-4307-9aaf-121d16ffd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_keywords = kws_counts.limit(20).toPandas()[\"K\"].tolist()\n",
    "selected_keywords = [format_kwd(K) for K in selected_keywords] + [\"None\"]\n",
    "#selected_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98642f8a-8529-46e8-acec-2830f7620938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.geeksforgeeks.org/python-intersection-two-lists/\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f908078-41cd-4d63-98c1-e6dd5c088636",
   "metadata": {},
   "source": [
    "Extracting these keywords to a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "638735b1-07da-4574-b2e1-34fd36ed317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns first keyword\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def list_keywords(a):\n",
    "    return [format_kwd(aa[\"value\"]) for aa in a] if a else [\"None\"]\n",
    "\n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def filtered_list_keywords(a):\n",
    "    kws_list = [format_kwd(aa[\"value\"]) for aa in a] if a else [\"None\"]\n",
    "    kws_list = intersection(kws_list, selected_keywords)\n",
    "    return kws_list if kws_list else [\"None\"]\n",
    "\n",
    "withKeywords = short_papers.withColumn(\"keywords_\", filtered_list_keywords(col(\"keywords\"))) # .withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01341d41-f203-4d73-bbd1-071bd49fb717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- number_of_pages: long (nullable = true)\n",
      " |-- citation_count: long (nullable = true)\n",
      " |-- num_refs: long (nullable = true)\n",
      " |-- numerical_calculations: integer (nullable = true)\n",
      " |-- numerical_calculations__interpretation_of_experiments: integer (nullable = true)\n",
      " |-- bibliography: integer (nullable = true)\n",
      " |-- quantum_chromodynamics: integer (nullable = true)\n",
      " |-- supersymmetry: integer (nullable = true)\n",
      " |-- CP__violation: integer (nullable = true)\n",
      " |-- neutrino__oscillation: integer (nullable = true)\n",
      " |-- Feynman_graph: integer (nullable = true)\n",
      " |-- electron_positron__annihilation: integer (nullable = true)\n",
      " |-- Feynman_graph__higher-order: integer (nullable = true)\n",
      " |-- effective_Lagrangian: integer (nullable = true)\n",
      " |-- neutrino__mixing_angle: integer (nullable = true)\n",
      " |-- neutrino__mass: integer (nullable = true)\n",
      " |-- neutrino__solar: integer (nullable = true)\n",
      " |-- perturbation_theory__higher-order: integer (nullable = true)\n",
      " |-- Higgs_particle__mass: integer (nullable = true)\n",
      " |-- electroweak_interaction: integer (nullable = true)\n",
      " |-- neutrino__cosmic_radiation: integer (nullable = true)\n",
      " |-- p_p__inclusive_reaction: integer (nullable = true)\n",
      " |-- critical_phenomena: integer (nullable = true)\n",
      " |-- None: integer (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dummy variables\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "dummy_df = withKeywords\n",
    "for kw in selected_keywords:\n",
    "    dummy_df = dummy_df.withColumn(\n",
    "        kw.replace(\".\", \"_\"), \n",
    "        array_contains(col(\"keywords_\"), kw).cast(\"int\")\n",
    "    )\n",
    "dummy_df = dummy_df.select( [\"title\",\"number_of_pages\", \"citation_count\", \"num_refs\"] + selected_keywords)\n",
    "dummy_df = dummy_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "dummy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3610286-d3f5-461c-83b3-3559505705af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.write.mode(\"overwrite\").json(\"../data/processed/\"+data_parh+\"/kws/dummy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2072f23e-82c3-450c-b20a-eb127fdb6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7dd5039-3d8d-4c24-8522-93266199f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=dummy_df.columns[1:10], outputCol=vector_col)\n",
    "vector_df = assembler.transform(dummy_df).select(vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55acb8f4-36e0-431a-be92-13ad7d4e68bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(corr_features=SparseVector(9, {0: 7.0, 2: -1.0, 7: 1.0, 8: 1.0})),\n",
       " Row(corr_features=SparseVector(9, {0: 11.0, 2: -1.0, 8: 1.0})),\n",
       " Row(corr_features=DenseVector([12.0, 4.0, -1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0])),\n",
       " Row(corr_features=SparseVector(9, {0: 2.0, 1: 15.0, 2: 6.0})),\n",
       " Row(corr_features=SparseVector(9, {0: 12.0, 1: 1.0, 2: -1.0, 7: 1.0})),\n",
       " Row(corr_features=SparseVector(9, {0: 322.0, 2: -1.0})),\n",
       " Row(corr_features=SparseVector(9, {0: 5.0, 1: 4.0, 2: 40.0})),\n",
       " Row(corr_features=DenseVector([6.0, 1.0, 35.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0])),\n",
       " Row(corr_features=SparseVector(9, {0: 6.0, 1: 1.0, 2: 8.0, 3: 1.0})),\n",
       " Row(corr_features=SparseVector(9, {0: 6.0, 1: 18.0, 2: 9.0}))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa52ecac-d6ce-48ab-bd26-4709eb607044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47f7b09c-401b-4deb-afbd-dc50117e552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mCorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Compute the correlation matrix for the input dataset of Vectors using the specified method.\n",
       "Methods currently supported: `pearson` (default), `spearman`.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Notes\n",
       "-----\n",
       "For Spearman, a rank correlation, we need to create an RDD[Double] for each column\n",
       "and sort it in order to retrieve the ranks and then join the columns back into an RDD[Vector],\n",
       "which is fairly costly. Cache the input Dataset before calling corr with `method = 'spearman'`\n",
       "to avoid recomputing the common lineage.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/ml/stat.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bcc7c337-7895-4187-a614-82a40e0ea358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.025303399965849505"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = dummy_df.columns[1:]\n",
    "dummy_df.stat.corr(cols[10], cols[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfe80c7c-605e-40ed-9c3a-88e46d3752e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.ml.stat.Correlation.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 51.0 failed 1 times, most recent failure: Lost task 3.0 in stage 51.0 (TID 136) (7aaf0255f268 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4188/0x0000000841533040: (struct<number_of_pages_double_VectorAssembler_79ffbb51380f:double,citation_count_double_VectorAssembler_79ffbb51380f:double,num_refs_double_VectorAssembler_79ffbb51380f:double,numerical_calculations_double_VectorAssembler_79ffbb51380f:double,numerical_calculations__interpretation_of_experiments_double_VectorAssembler_79ffbb51380f:double,bibliography_double_VectorAssembler_79ffbb51380f:double,quantum_chromodynamics_double_VectorAssembler_79ffbb51380f:double,supersymmetry_double_VectorAssembler_79ffbb51380f:double,CP__violation_double_VectorAssembler_79ffbb51380f:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:441)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:90)\n\tat org.apache.spark.ml.stat.Correlation$.corr(Correlation.scala:71)\n\tat org.apache.spark.ml.stat.Correlation.corr(Correlation.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4188/0x0000000841533040: (struct<number_of_pages_double_VectorAssembler_79ffbb51380f:double,citation_count_double_VectorAssembler_79ffbb51380f:double,num_refs_double_VectorAssembler_79ffbb51380f:double,numerical_calculations_double_VectorAssembler_79ffbb51380f:double,numerical_calculations__interpretation_of_experiments_double_VectorAssembler_79ffbb51380f:double,bibliography_double_VectorAssembler_79ffbb51380f:double,quantum_chromodynamics_double_VectorAssembler_79ffbb51380f:double,supersymmetry_double_VectorAssembler_79ffbb51380f:double,CP__violation_double_VectorAssembler_79ffbb51380f:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 36 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCorrelation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectot_col\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/stat.py:165\u001b[0m, in \u001b[0;36mCorrelation.corr\u001b[0;34m(dataset, column, method)\u001b[0m\n\u001b[1;32m    163\u001b[0m javaCorrObj \u001b[38;5;241m=\u001b[39m _jvm()\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mml\u001b[38;5;241m.\u001b[39mstat\u001b[38;5;241m.\u001b[39mCorrelation\n\u001b[1;32m    164\u001b[0m args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m (dataset, column, method)]\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mjavaCorrObj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.ml.stat.Correlation.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 51.0 failed 1 times, most recent failure: Lost task 3.0 in stage 51.0 (TID 136) (7aaf0255f268 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4188/0x0000000841533040: (struct<number_of_pages_double_VectorAssembler_79ffbb51380f:double,citation_count_double_VectorAssembler_79ffbb51380f:double,num_refs_double_VectorAssembler_79ffbb51380f:double,numerical_calculations_double_VectorAssembler_79ffbb51380f:double,numerical_calculations__interpretation_of_experiments_double_VectorAssembler_79ffbb51380f:double,bibliography_double_VectorAssembler_79ffbb51380f:double,quantum_chromodynamics_double_VectorAssembler_79ffbb51380f:double,supersymmetry_double_VectorAssembler_79ffbb51380f:double,CP__violation_double_VectorAssembler_79ffbb51380f:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:441)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:90)\n\tat org.apache.spark.ml.stat.Correlation$.corr(Correlation.scala:71)\n\tat org.apache.spark.ml.stat.Correlation.corr(Correlation.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4188/0x0000000841533040: (struct<number_of_pages_double_VectorAssembler_79ffbb51380f:double,citation_count_double_VectorAssembler_79ffbb51380f:double,num_refs_double_VectorAssembler_79ffbb51380f:double,numerical_calculations_double_VectorAssembler_79ffbb51380f:double,numerical_calculations__interpretation_of_experiments_double_VectorAssembler_79ffbb51380f:double,bibliography_double_VectorAssembler_79ffbb51380f:double,quantum_chromodynamics_double_VectorAssembler_79ffbb51380f:double,supersymmetry_double_VectorAssembler_79ffbb51380f:double,CP__violation_double_VectorAssembler_79ffbb51380f:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 36 more\n"
     ]
    }
   ],
   "source": [
    "Correlation.corr(vector_df, vectot_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa6e74-cc96-41b7-8440-4a6d54286ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
